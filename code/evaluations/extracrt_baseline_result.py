import os
import re
import queue
import shutil
import logging
import subprocess
from bs4 import BeautifulSoup

import tools.io_utils as io_utils
from procedure.post_process import check_class_name
from evaluations.coverage_test import ProjrctTestRunner, CoverageExtractor


def check_method_name(method_name, target):
    while len(re.findall(r"<[^<>]*>", target, flags=re.DOTALL))>0:
        target = re.sub(r"<[^<>]*>", "", target, flags=re.DOTALL)
    method_parts = method_name.replace("(", "( ").replace(")", " )").split()
    target_parts = target.replace("(", "( ").replace(")", " )").split()
    if len(method_parts) != len(target_parts):
        return False
    if method_parts[0] != target_parts[0]:
        return False
    for item_m, item_t in zip(method_parts[1:-1], target_parts[1:-1]):
        if item_m == "Object" or item_m == "Object,": continue
        if "." in item_m: item_m = item_m.split(".")[-1]
        if "." in item_t: item_t = item_t.split(".")[-1]
        elif item_m != item_t:
            return False
    return True


def extract_coverage_html(html_path, method):
    logger = logging.getLogger(__name__)
    logger.info(f"Extracting coverage form {html_path}, method: {method}")
    coverage_score = None
    if not os.path.exists(html_path):
        logger.exception(f"report file not found: {html_path}")
        return coverage_score
    with open(html_path, "r") as file:
        soup = BeautifulSoup(file, 'lxml-xml')
    for tr in soup.find_all(name='tbody')[0].find_all(name='tr', recursive=False):
        tds = tr.contents
        try:
            method_name = tds[0].span.string
        except AttributeError:
            method_name = tds[0].a.string
        if check_method_name(method_name, method):
            instruction_cov = float(tds[2].string.replace("%", ""))/100
            branch_cov = float(tds[4].string.replace("%", ""))/100
            coverage_score = {"inst_cov": instruction_cov, "bran_cov": branch_cov}
            break
    return coverage_score


def count_general_metrics(summary:dict):
    # case_num = 0
    # compile_num = 0
    # pass_num = 0
    tfunc_num = 0
    inst_cov = 0.0
    bran_cov = 0.0
    for _, item in summary.items():
        if "inst_cov" in item and not isinstance(item["inst_cov"],str):
            tfunc_num += 1
            inst_cov += item["inst_cov"]
            bran_cov += item["bran_cov"]
    summary.update({
        # "compile_pass_rate": compile_num/case_num if case_num > 0 else 0,
        # "execution_pass_rate": pass_num/case_num if case_num > 0 else 0,
        "average_instruction_coverage": inst_cov/tfunc_num if tfunc_num > 0 else 0.0,
        "average_branch_coverage": bran_cov/tfunc_num if tfunc_num > 0 else 0.0
    })
    return summary


def extract_coverage_HITS(result_folder, dataset_info, dataset_meta, save_path):
    if not os.path.exists(save_path): os.makedirs(save_path)
    logger = logging.getLogger(__name__)

    for meta_info in dataset_meta:
        pj_name = meta_info["project_name"]
        pj_info  = dataset_info[pj_name]
        name_to_idx = meta_info["method_name_to_idx"]
        project_result = f"{result_folder}/{pj_name}/methods"
        project_coverage = {}
        for tinfo in pj_info["focal-methods"]:
            method_name = tinfo["method-name"]
            target_class = tinfo["class"]
            msig = target_class + "." + method_name
            method_idx = name_to_idx[msig]
            package = tinfo["package"]
            class_name = target_class.split(".")[-1]
            coverage_path = f"{project_result}/{method_idx}/full_report/{package}/{class_name}.html"
            coverage_score = extract_coverage_html(coverage_path, method_name)
            if coverage_score is None:
                logger.exception(f"coverage score not found: {coverage_path}")
                coverage_score = {"inst_cov": 0, "bran_cov": 0}
            project_coverage[f"{target_class}#{method_name}"] = coverage_score
        project_coverage = count_general_metrics(project_coverage)
        io_utils.write_json(f"{save_path}/{pj_name}.json", project_coverage)
    return


def set_file_structure(report_path, dataset_info):
    for pj_name, pj_info in dataset_info.items():
        report_folder = report_path.replace("<project>", pj_name)
        report_csv = f"{report_folder}/jacoco-report-csv/"
        io_utils.check_path(report_csv)
        
        for test_info in pj_info["focal-methods"]:
            id = test_info["id"]
            report_html = f"{report_folder}/jacoco-report-html/{id}/"
            io_utils.check_path(report_html)
    pass


def extract_coverage_ChatUniTest(result_folder, dataset_info, fstruct, task_setting):
    root_path = os.getcwd().replace("\\", "/")
    dataset_dir = f"{root_path}/{fstruct.DATASET_PATH}"
    testclass_path = f"{result_folder}/<project>/test_classes/"
    report_path = f"{root_path}/{result_folder}/<project>/reports/"
    dependency_dir = f"{root_path}/{fstruct.DEPENDENCY_PATH}"
    compile_test = task_setting.COMPILE_TEST
    projects = task_setting.PROJECTS
    select = True if len(projects)>0 else False
    logger = logging.getLogger(__name__)
    set_file_structure(report_path, dataset_info)

    # for root, dirs, files in os.walk(result_folder):
    #     for file in files:
    #         if file.endswith(".java"):
    #             file_path = os.path.join(root, file)
    #             new_file_name = re.sub(r"_[0-9]+_[0-9]+", "", file)
    #             new_file_path = os.path.join(root, new_file_name)
    #             if new_file_name == file: continue
    #             logger.info(f"renaming {file_path} to {new_file_path}")
    #             try:
    #                 os.rename(file_path, new_file_path)
    #             except FileExistsError:
    #                 logger.warning(f"file {new_file_path} already exists")
    #                 continue

    for pj_name, info in dataset_info.items():
        if select and pj_name not in projects: continue
        project_path = f"{dataset_dir}/{info['project-url']}"
        info["project-url"] = project_path
        # run converage test & generate report
        runner = ProjrctTestRunner(info, dependency_dir, testclass_path, report_path)
        test_result = runner.run_project_test(compile_test)
        logger.info(test_result)
        # extract coverage
        extractor = CoverageExtractor(info, report_path)
        coverage_data = extractor.generate_project_summary(test_result)
        logger.info(f"report data:\n{coverage_data}")
        coverage_file = f"{report_path}/summary.json".replace("<project>", pj_name)
        io_utils.write_json(coverage_file, coverage_data)
    return


class HITSRunner(ProjrctTestRunner):
    def __init__(self, project_info, dependency_dir, testclass_path, report_path):
        super().__init__(project_info, dependency_dir, testclass_path, report_path)
        self.check_report_path()
    
    def check_report_path(self):
        io_utils.check_path(self.report_path)
        test_objects = self.project_info["focal-methods"]
        report_csv = f"{self.report_path}/jacoco-report-csv/"
        io_utils.check_path(report_csv)
        for tobject in test_objects:
            testid = tobject["id"]
            report_html = f"{self.report_path}/jacoco-report-html/{testid}/"
            io_utils.check_path(report_html)

    def check_testclass_name(self):
        # 遍历 testclass_path下的所有文件
        dir_list = queue.Queue()
        dir_list.put(self.testclass_path)
        while not dir_list.empty():
            current_dir = dir_list.get()
            paths = os.listdir(current_dir)
            for path in paths:
                full_path = os.path.join(current_dir, path)
                if os.path.isdir(full_path):
                    dir_list.put(full_path)
                elif path.endswith(".java") and "slice" in path:
                    self.logger.info(f"Checking class name in {full_path}")
                    class_name = path.replace(".java", "")
                    if class_name is None:
                        self.logger.error(f"Invalid class name in {file_path}")
                        continue
                    file_path = os.path.join(current_dir, path)
                    code = io_utils.load_text(file_path)
                    code = check_class_name(code, class_name)
                    io_utils.write_text(file_path, code)


    def run_project_test(self, compile_test=True):
        project_name = self.project_info["project-name"]
        project_url = self.project_info["project-url"]
        test_objects = self.project_info["focal-methods"]
        self.test_result = {}
        
        # copy test classes to the project directory
        test_dir = f"{project_url}/src/test/java/"
        if os.path.exists(test_dir):
            shutil.rmtree(test_dir, ignore_errors=False)
        io_utils.copy_dir(self.testclass_path, test_dir)
        self.logger.info(f"Running tests for project: {project_name}")

        for tobject in test_objects:
            # test_class = tobject["test-class"]
            test_path = tobject["test-path"]
            testid = tobject["id"]
            method = tobject["method-name"]
            package = tobject["package"]
            test_folder = "/".join(test_path.split("/")[:-1])

            matched_files = []
            if os.path.exists(f"{project_url}/{test_folder}"):
                for file in os.listdir(f"{project_url}/{test_folder}"):
                    if file.startswith(testid): #and file.find("slice")==-1:
                        class_path = f"{project_url}/{test_folder}/{file}"
                        matched_files.append(class_path)

            data_id = f"{tobject['class']}#{method}"
            self.test_result[data_id] = {}
            if len(matched_files) == 0:
                self.test_result[data_id]["error_type"] = "compile error"
                self.test_result[data_id]["test_cases"] = 1
                self.test_result[data_id]["passed_cases"] = 0
                self.test_result[data_id]["note"] = "test class not found"
                continue
            compiled_files = self.compile_test_group(matched_files)
            if len(compiled_files) == 0:
                self.test_result[data_id]["error_type"] = "compile error"
                self.test_result[data_id]["test_cases"] = len(matched_files)
                self.test_result[data_id]["passed_cases"] = 0
                continue
            if not self.run_test_group(package, testid, data_id):
                self.test_result[data_id]["error_type"] = "execution error"
                self.test_result[data_id]["test_cases"] = len(matched_files)
                continue
            if not self.generate_report_single(testid):
                self.test_result[data_id]["error_type"] = "report error"
        return self.test_result
    
    def compile_test_group(self, class_path):
        sucess = []
        for file in class_path:
            if super().compile_test(file):
                sucess.append(file)
        return sucess

    def run_test_group(self, package, testid, data_id):
        """
        Run a group of tests in the specified test class.
        """
        test_dependencies = f"libs/*;target/test-classes;target/classes;{self.dependency_fd}/*"
        java_agent = f"-javaagent:{self.dependency_fd}/jacocoagent.jar=destfile=target/jacoco.exec"
        testid = testid.replace("$", "\\$")
        test_cmd = ['java', '-cp', test_dependencies, java_agent, 'org.junit.platform.console.ConsoleLauncher', '--disable-banner', '--disable-ansi-colors', '--fail-if-no-tests', '--select-package', package, '--include-classname', f".*{testid}.*"]
        script = self.cd_cmd + test_cmd
        self.logger.info(' '.join(test_cmd))
        result = subprocess.run(script, capture_output=True, text=True, shell=True, encoding="utf-8", errors='ignore')
        test_info = result.stdout
        if result.returncode == 0:
            self.logger.info(f"test execution info: {test_info}")
            test_cases, passed_cases = self.get_pass_rate(test_info)
            self.test_result[data_id]["test_cases"] = test_cases
            self.test_result[data_id]["passed_cases"] = passed_cases
            return True
        elif test_info.find("Test run finished")!=-1:
            self.logger.warning(f"return code: {result.returncode}")
            self.logger.warning(f"test case failed in {testid}, info:\n{result.stderr}\n{test_info}")
            test_cases, passed_cases = self.get_pass_rate(test_info)
            self.test_result[data_id]["test_cases"] = test_cases
            self.test_result[data_id]["passed_cases"] = passed_cases
            if test_cases == 0: return False
            return True
        else:
            self.logger.error(f"error occured in execute test class {testid}, info:\n{result.stderr}\n{test_info}")
            return False

def run_HITS_coverage(fstruct, task_setting, result_folder, dataset_info):
    root_path = os.getcwd().replace("\\", "/")
    dataset_dir = f"{root_path}/{fstruct.DATASET_PATH}"
    testclass_path = f"{result_folder}/<project>/test-classes/"
    report_path = f"{root_path}/{result_folder}/<project>/reports/"
    dependency_dir = f"{root_path}/{fstruct.DEPENDENCY_PATH}"
    compile_test = task_setting.COMPILE_TEST
    projects = task_setting.PROJECTS
    select = True if len(projects)>0 else False
    logger = logging.getLogger(__name__)

    for pj_name, info in dataset_info.items():
        if select and pj_name not in projects: continue
        project_path = f"{dataset_dir}/{info['project-url']}"
        info["project-url"] = project_path
        # run converage test & generate report
        runner = HITSRunner(info, dependency_dir, testclass_path, report_path)
        runner.check_testclass_name()
        test_result = runner.run_project_test(compile_test)
        logger.info(test_result)
        # extract coverage
        extractor = CoverageExtractor(info, report_path)
        coverage_data = extractor.generate_project_summary(test_result)
        logger.info(f"report data:\n{coverage_data}")
        coverage_file = f"{report_path}/summary.json".replace("<project>", pj_name)
        io_utils.write_json(coverage_file, coverage_data)
    return


def exract_baseline_coverage(file_structure, task_setting, benchmark, dataset_info):
    dataset_path = file_structure.DATASET_PATH
    baseline_path = benchmark.BASELINE_PATH
    selected_baselines = benchmark.BASELINES
    logger = logging.getLogger(__name__)

    # extract HITS coverage
    if "HITS" in selected_baselines:
        logger.info("Extracting HITS coverage...")
        # dataset_meta = io_utils.load_json(f"{dataset_path}/dataset_meta.json")
        # HITS_result = "../../../paper-repetition/HITS-rep/playground_check_official"
        # HITS_save = f"{baseline_path}/HITS"
        # extract_coverage_HITS(HITS_result, dataset_info, dataset_meta, HITS_save)

        # run HITS coverage test
        HITS_result = f"{baseline_path}/HITS"
        run_HITS_coverage(file_structure, task_setting, HITS_result, dataset_info)
    
    # extract ChatUniTest coverage
    if "ChatUniTest" in selected_baselines:
        logger.info("Extracting ChatUniTest coverage...")
        chatunitest_result = f"{baseline_path}/ChatUniTest"
        extract_coverage_ChatUniTest(chatunitest_result, dataset_info, file_structure, task_setting)
    return