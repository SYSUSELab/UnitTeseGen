class FileStructure:
    """
    File structure settings
    """
    ROOT_PATH = "Path/to/Unit-Test-Generation/code" # absolute path
    DEPENDENCY_PATH = "./dependencies"
    DATASET_PATH = "../dataset/projects"
    CODE_INFO_PATH = "../dataset/project_index"
    # "<project>" will be replaced by the project name in the execution process
    PROMPT_PATH = "../evaluation/<project>/context+prompts"
    FIX_PATH = "../evaluation/repair_singal/<project>/fix"
    RESPONSE_PATH = "../evaluation/<project>/responses"
    TESTCLASSS_PATH = "../evaluation/<project>/test_classes"
    REPORT_PATH = "../evaluation/<project>/reports"

class LLMSettings:
    """
    LLM settings
    """
    MODEL = "deepseek-v3-0324"
    API_ACCOUNTS = [
        {   
            "base_url":"",
            "api_key":"xxx",
        }
    ]

class TaskSettings:
    """
    Task settings
    """
    # project selection
    # for projects and cases list, use empty list [] to select all
    PROJECTS = ['batch-processing-gateway', 'commons-cli', 'commons-codec', 'commons-collections', 'commons-csv', 'datafaker', 'gson', 'jdom2', 'ruler', 'windward']
    # test class selection, list of id in dataset, e.g. ["SparkPodSpec_copyFrom"]
    CASES_LIST = []
    # prompt selection
    PROMPT_LIST = ['condition','io','exception']
    SAVE_INTER_RESULT = True
    COMPILE_TEST = True
    MAX_WORKERS = 8
    # top k for similarity search
    SIM_TOP_K = "10"

class BecnchMark:
    """
    Benchmark results
    """
    BASELINE_PATH = "../evaluation/baseline"
    HITS_RESULTS = ""